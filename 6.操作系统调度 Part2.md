# 四.多核调度策略

任务可以同时在多个CPU上并行执行，此时调度器要考虑的问题就变成了：

* Which：当前应当选择哪个任务进行调度？
* Where：每个调度的任务应当在哪个CPU核心上执行？
* How many：每个调度的任务应当执行多久？

## 1.负载分担策略

实际上就是简单化处理Where这个问题，维护一个全局运行队列，当某个CPU需要调度任务的时候，直接**根据给定的调度策略**，从全局运行队列中选择一个任务。

<img src="图片\调度13.jpg" style="zoom:80%;" />

优点：

* 简单
* 每个CPU都会分担系统的负载，不会出现CPU资源浪费的情况。

问题：

* 多核共享一个全局运行队列的**同步开销**
* 运行在不同CPU之上的任务（或者是细化之后的子任务）可能具有**依赖关系**：数据上的需要（比如 $.c \rightarrow .o \rightarrow .exe$ 这种C文件的编译过程），以及倾向于同步执行的关联任务。
* 任务在不同CPU核心之间进行切换有可能会导致**大量开销**，包括重新载入内存、TLB刷新等，对缓存不太友好。

## 2.考虑依赖关系

### （1）协同调度

####①内容

1. 尽量避免同时调度**有依赖关系的任务**
2. 提高**关联任务**（比如因为通信需求而倾向于同时调度的任务）的效率。

> 区分：有依赖关系指的是人物之间具有逻辑上的先后顺序，相互依赖；关联任务指的是任务之间具有通信需求。两者一个排斥同时进行，一个倾向于同时进行。

#### ②适用场景

协同调度适合并行计算(parallel computing)场景，也即将任务切分成多个并行的子任务的场景（创建并执行大量关联任务，并且各组关联任务之间有依赖关系）。

并行计算场景下的整体同步并行(Bulk Synchronous Parallelism, BSP) 计算模型：

* **并发计算**：每个CPU核心独立计算自己的子任务
* **通信**：CPU核心之间通过通信交换数据
* **同步**：设置屏障点，先到达屏障点的CPU需要等待其它CPU也到达此点，并且通过通信进行同步之后，才能执行接下来的代码逻辑。

<img src="图片\调度14.jpg" style="zoom:80%;" />

### （2）群组调度

**把没有依赖关系的子任务分到同一组，尽量使得关联性任务分到同一组**，这样能够达到组内任务并行执行的效果，不同组之间的依赖关系也可以通过组间切换时的通信进行解决。

<img src="图片\调度15.jpg" style="zoom:80%;" />

## 3.考虑切换开销

###（1）背景

对应负载分担策略下的第三个问题，也就是某个任务在多个CPU间进行切换，对缓存不太友好，开销过大。

<img src="图片\调度20.jpg" style="zoom:80%;" />

### （2）缓存友好型调度：两级调度

全局调度器会使得任务在不同的CPU核心上切换执行，为了减少切换开销，我们思考能否尽可能的让一个任务在一个CPU核心上执行？

将调度器分为两级：

1. 全局调度器

   以任务为粒度，根据各个CPU的负载情况，为各个需要调度的任务分配一个CPU核心，指定其在此核心上运行。

2. 本地调度器

   可以按照任意单核调度策略进行调度。

<img src="图片\调度21.jpg" style="zoom:80%;" />

优点：

* 线程无须在CPU之间来回切换，**提高了缓存的局部性**，减少了数据竞争的冲突。
* 将单核调度策略与多核调度进行解耦，可以分别进行精进。

缺点：

* 容易产生**负载不均衡**的现象（任务量有大有小，以任务为粒度进行一级调度的代价）。

  <img src="图片\调度22.jpg" style="zoom:80%;" />

> Linux操作系统会为每个CPU维护一个本地运行队列，也就相当于一个本地调度器。

## *4.负载均衡与负载追踪

### （1）负载均衡

由于任务的特点不一样，很难根据任务数量等一些特征来代表真实负载，需要在实际场景中追踪CPU的负载情况，并将任务从负载高的CPU迁移到负载低的CPU。

<img src="图片\调度23.jpg" style="zoom:80%;" />

### （2）负载追踪

####①以运行队列为粒度追踪负载

将负载与**运行队列的长度**挂钩。

但从一个高负载的运行队列中，到底选取哪个任务迁移呢？没有相关信息，因此不够准确

####②以调度实体为粒度追踪负载

也即以调度实体（单个任务）为粒度记录负载。

**PELT(Per Entity Load Tracking，从Linux 3.8开始)**：

其重点就是**量化每个调度实体对负载贡献**的过程，我们可以通过计算调度实体在某个特定的周期（$1024 \mu s$）中处于可运行状态的时间 $x$ 来描述这个量。

线程 $t$ 在第 $i$ 个周期内的负载为：
$$
L_{t,i}=\frac x{1024} \cdot CpuScaleFactor
$$
PS ： $CpuScaleFactor$ 为描述线程所依托的CPU的特征的量。之所以需要这一系数是因为，在不同CPU上相同的某一占有率的价值实际上是不一样的，故而性能高的CPU，Factor也更高。

之后我们就可以得到任务 $t$ 的动态变化的总负载：
$$
Load_t= 𝐿_{𝑡,𝑖} + 𝛾 ⋅ 𝐿_{𝑡,𝑖}−1 + 𝛾^2 ⋅ 𝐿_{𝑡,𝑖−2} + ⋯ + 𝛾^𝑖𝐿_{t,0}
$$

# 五.调度进阶机制

前面主要介绍了操作系统如何更好的利用资源、高效的完成响应的请求，其始终是站在一个“场外”的角度来不断优化所谓的调度策略的。其像一个大家长，统筹管理所有任务的调度方式。但是由于应用程序的场景是十分动态而且多元的，而OS根据已有的信息很难兼顾所有任务，并且难以预测准确。

故而OS为应用程序提供了接口，使得其可以主观上调节自身的调度行为，更好的满足自身的需求。

> 老程序猿比内核更懂得该如何调度自己写的程序

## 1.处理器亲和性

允许程序对任务可以使用的CPU核心进行调度。

~~~c
#include <sched.h>

int main() {
cpu_set_t mask; //表征可以执行任务的CPU核心的集合，每一位对应一个CPU的核心。

CPU_ZERO(&mask);// 初始化 mask 的CPU集合为空
    
// 在 mask 的 CPU 集合中加入逻辑核心0和逻辑核心2
CPU_SET(0, &mask);
CPU_SET(2, &mask);
    
// 根据 mask 设置当前任务的亲和性
sched_setaffinity(0, sizeof(mask), &mask);
}
~~~

## 2.调度策略设置

前面是规定任务可以在哪个CPU上运行，这里是指定任务应当使用怎样的调度策略。

Linux调度器：

* 截止时间调度器（Deadline Scheduler, DL）

  * SCHED_DEADLINE——类似EDF

* 实时调度器（Real Time Scheduler, RT）

  * SCHED_FIFO——执行直至结束或者被更高优先级的任务抢占

  * SCHED_RR——执行一定时间片后不再执行

* 完全公平调度器（Completely Fair Scheduler, CFS）

  * SCHED_OTHER——公平的分时调度策略
  * SCHED_BATCH——针对不会与用户交互的批处理任务
  * SCHED_IDLE——针对优先级最低的后台任务

  























