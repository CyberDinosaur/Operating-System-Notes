# 一.概述

## 1.为什么需要系统虚拟化？

计算某种程度上也是一种资源，支撑现代社会的各种活动，比如开网站需要算力、搞科研需要算力。这就导致后PC时代计算资源愈发集中，并且作为一种服务提供给用户，也就是所谓的**基础设施即服务**（Infrastructure as a Service, IaaS）。

为了说明IaaS的必要性，我们考虑一个栗子。张三开设了一个叫薇博的网站，其必然需要一定的计算资源。张三首先可以选择自己去购买服务器，但是这就会面临一个窘境：

* 服务器买少了，高峰来了直接崩溃
* 服务器买多了，空闲的计算资源怎么办？

云计算应运而生：资源共享、时分复用。

<img src="图片\设备管理13.jpg" style="zoom:80%;" />

集中式的计算资源内、也即一个IaaS服务器一般服务多个用户，**各个用户对于体系结构的需求不同，**系统虚拟化技术就显得十分必要了。

## 2.虚拟化优势

* **服务器整合**

  单个物理机利用率低，可以利用系统虚拟化进行资源整合。基于用户错峰使用的特性，甚至可以超售赚钱。

* **方便程序开发**

  调试操作系统（单步调试、查看或者随时修改当前虚拟硬件状态等等）

  在不同OS上测试应用程序的兼容性

* **简化服务器管理**

  虚拟机热迁移，通过软件接口管理虚拟机

## 3.什么是系统虚拟化？

### （1）定义操作系统

学了这么长时间的操作系统，我们始终没有对其进行精确定义。无论是管理硬件并向上提供功能性接口的“小管家”，还是CPU的“驱动程序”实际上都是很模糊的描述。下面我们通过三种层次的接口，提供一种新的OS定义方式。

<img src="图片\系统虚拟化01.jpg" style="zoom:80%;" />

“操作系统”实际上就是通过ISA接口和硬件(Machine)分开的部分，换句话说就是，**ISA提供了操作系统和硬件系统之间的分界线**。

### （2）虚拟机和虚拟机监视器

系统虚拟化技术能在一台物理主机上创建多个**虚拟机**。所谓的虚拟机实际上就是一种操作系统，其同样通过ISA直接与“硬件”交互，只不过虚拟机的“硬件”是虚拟化的。

硬件的虚拟由**虚拟机监视器**（Virtual Machine Monitor, VMM）提供，其运行在CPU的最高特权级，直接控制着硬件，为上层软件提供**虚拟的硬件接口**。

<img src="图片\系统虚拟化02.jpg" style="zoom:80%;" />



VMM上可以运行多台虚拟机，其根据不同虚拟机的硬件需求，提供不同的ISA接口。

### （3）VMM的分类

<img src="图片\系统虚拟化03.jpg" style="zoom:80%;" />

实际上对于OS、VM、VMM来说，其都认为自己直接与硬件接触，这也是所谓的泛操作系统的定义。但是很多情况下其连接的都是虚拟的硬件。比如Type-1中的各个虚拟机；Type-2中的VMM，其可以复用宿主操作系统的大部分功能。

###（4）系统虚拟化的标准

1. VMM为VM内的程序**提供与该程序原先执行的硬件完全一样的接口**。这意味着所有能运行在物理主机上的程序都可以在虚拟机上运行。
2. **虚拟机只比在无虚拟化的情况下性能略差一点**。也即虚拟的硬件接口这一层抽象不会过分影响OS的性能。
3. **VMM能控制所有的物理系统资源**。

这三点标准是我们实现系统虚拟化的前提条件。

# 二.系统虚拟化的实现

系统虚拟化的过程中使VMM获得硬件资源的绝对管理权限实际上不难，难点在于如何在只有一套硬件设施的基础下对不同VM提供不同的ISA。

我们显然可以通过**时分复用**的方式对有限的硬件资源进行划分，并将其提供给不同的VM（提供的过程可能还会涉及到调度）。当VM尝试对自身的虚拟硬件资源进行相应的修改的时候，只需要让大管家VMM在真实硬件资源上进行相应的操作，然后修改对应VM的虚拟硬件状态即可。

上面一段话我们貌似已经解决了系统虚拟化问题，但是在让VMM在分给对应VM的硬件资源片上进行相应操作的时候，我们忽略了硬件很多都是临界资源，虚拟化处理不当可能会导致各个VM之间相互影响。所以系统虚拟化技术实现的难点在于，**VMM进行硬件操作的时候，如何实现不同VM从属硬件资源的隔离**。

<img src="图片\系统虚拟化04.jpg" style="zoom:80%;" />

## 1.系统虚拟化的流程

1. 第一步

   捕捉所有VM尝试进行的系统ISA并**下陷(Trap)**

2. 第二步

   由具体指令实现相应虚拟化硬件操作

   * 处理器虚拟化
     * 捕捉系统ISA
     * 控制虚拟处理器的行为
   * 内存虚拟化
     * 提供“假”物理内存的抽象
   * 设备虚拟化
     * 提供虚拟的I/O设备

3.  第三步.

   回到虚拟机继续执行

## 2.CPU虚拟化

VMM直接运行在物理主机上，使用物理ISA，并向上层VM提供虚拟ISA。

上层VM在运行的时候会执行许多指令，为了更好的分析VMM处理这些指令的过程，我们先对指令进行划分。

###（1）敏感指令

指**管理系统物理资源或更改CPU状态的指令**==？==。通常包括：

* 读写特殊寄存器或执行特殊指令以**更改CPU状态**。

  例如在x86架构中修改CR0或CR4寄存器，在ARM架构中修改SCTRL_EL1寄存器；在x86架构中执行hlt指令，在AArch64中执行wfi指令。

* **读写敏感内存**

  如读写未映射的内存，写入只读页

* **执行I/O指令**

  如x86架构中的in和out指令等。

假设上层VM**不会执行敏感指令**，CPU虚拟化的逻辑就十分简单：

* 如果虚拟ISA和物理ISA相同，则直接在物理主机上运行
* 如果两者不同，则VMM需要进行软件模拟，翻译为物理ISA上的指令。之后同样直接在物理主机上跑就可以。

但是问题在于，VM**时常运行一些敏感指令**。由于敏感指令处理不好就意味着不同VM之间的硬件资源会产生交涉（这也就是为什么它们敏感的原因），所以VMM需要以一种安全的方式运行这些指令。

**所以为了完成CPU虚拟化，我们只需要让VMM学会分辨敏感指令与不敏感指令即可！**

###（2）可虚拟化架构

首先最简单的情况就是**所有的敏感指令都是特权指令**，也即敏感操作都要在虚拟内核态进行。当VM执行敏感指令的时候，其会尝试进入虚拟EL1特权级。VMM会捕捉到这种尝试进入虚拟EL1特权级的行为，并进行**下陷（trap）**。之后进行**模拟（emulate）**操作即可。

> 下陷：在用户态EL0执行的特权指令将陷入EL1的虚拟机监控器中。
>
> 模拟：这些指令的功能都由VMM内的函数安全地实现。
>
> 所谓的特权指令就是指在用户态执行时会触发下陷的指令。包括主动触发下陷的指令svc以及允许在用户态执行的指令（如写入只读内存），也即所谓的**系统调用**和**异常**。

也即如果能够做到下陷和模拟，CPU的虚拟化就成功实现了。我们将这种敏感指令都是特权指令的架构称为**可虚拟化架构**。

如果不满足这条定义，也即存在某些敏感指令不是特权指令的情况，即为**不可虚拟化架构**，此时VMM将很难通过检测尝试进入虚拟EL1的方式捕捉敏感指令。

### （2）不可虚拟化架构

####①解释执行

很简单，就是不管三七二十一，猜不出来我就不猜，不依赖于下陷，所有的指令都被VMM模拟执行。

<img src="图片\系统虚拟化05.jpg" style="zoom:80%;" />

* 依次取出虚拟机内的每一条指令，用软件方法对虚拟机代码进行模拟（既能保障安全，又能实现跨ISA的通用性）。
* 使用内存维护虚拟机状态。

优点：

* 解决了不可虚拟化架构下某些敏感指令不下陷的问题
* 易于实现，方法本身的复杂度较低
* 可以模拟不同ISA的虚拟机

缺点：

* 不加区分的把所有虚拟机的指令转化成多条模拟指令，会给虚拟机的执行带来巨大的性能开销。

####②二进制翻译

在解释执行的基础上提出了两个加速技术：

* 在执行前**批量翻译**虚拟机指令
* **缓存**已翻译完成的指令

使用**基本块**(Basic Block)的翻译粒度，每一个基本块被翻译完之后都被以**代码补丁**的形式缓存起来，在下一次翻译模拟之前，可以先去缓存中查询有无匹配项。

<img src="图片\系统虚拟化06.jpg" style="zoom:80%;" />

动态二进制翻译的缺点：

* 不能处理自修改的代码(Self modifying Code)

* 中断插入粒度变大

  模拟执行可以在任意指令位置插入虚拟中断，而二进制翻译时只能在基本块边界插入虚拟中断

####③半虚拟化

前面两种方案实际上都是**全虚拟化技术**，也即不能修改客户虚拟机的源代码，必须在机器指令层面进行模拟或翻译。

**半虚拟化技术**需要对客户操作系统和VMM进行协同设计：

* 让VMM提供接口给虚拟机，称为超级调用Hypercall
* 修改操作系统源码，让其主动调用VMM接口

也即**将所有不引起下陷的敏感指令都替换为超级调用**。

> 在全虚拟化技术中，客户虚拟机实际上是感知不到虚拟化环境的存在的，其以为自身就运行在最高的特权级。但是在半虚拟化技术中，客户虚拟机不仅知道自己运行在虚拟化环境中，而且还要对此进行相应的修改。

半虚拟化技术的优缺点：

* 优点
  * **带来更高的性能**：无须模拟运行敏感指令；可以减少冗余的代码逻辑、数据拷贝、特权级切换等操作（打开天窗说亮话）
  * **缓解语义鸿沟问题**：允许VMM获取虚拟机内部的状态，因此可以进一步提高资源的分配效率。
* 缺点
  * 需要修改操作系统代码，难以用于闭源系统，比如Windows
  * 即使是开源系统，也难以同时在不同版本中实现

####④硬件虚拟化（改硬件)

x86和ARM都引入了全新的虚拟化特权级。

##### A.x86

<img src="图片\系统虚拟化07.jpg" style="zoom:80%;" />

x86引入了root模式和non-root模式，VMM运行在root模式，虚拟机运行在non-root模式。两个模式内都有4个特权级别：Ring0~Ring3。

Intel VT-x为每个虚拟机提供了一个**虚拟机控制结构**（Virtual Machine Control Structure, VMCS），其是VMM提供给硬件的内存页（4KB），记录与当前VM运行相关的所有状态。VMM通过配置VMCS来管理虚拟机的内存映射和其它行为。

* VM Entry

  硬件自动将当前CPU中的VMM状态保存至VMCS

  硬件硬件自动从VMCS中加载VM状态至CPU中 

* VM Exit

  硬件自动将当前CPU中的VM状态保存至VMCS

  硬件自动从VMCS加载VMM状态至CPU中

##### B.ARM

<img src="图片\系统虚拟化08.jpg" style="zoom:80%;" />

上面的x86架构实际上就是使用一种第三方架构VMCS作为中间过程，在切换的过程中使用，可以起到控制对应VM的作用。

而在ARM架构的硬件虚拟化中，硬件为EL1和EL2提供了两套系统寄存器。运行在EL2的VMM可以随时读写EL0与EL1的寄存器。在VM下陷时，其系统寄存器的状态无须保存（x86则都需要保存到VMCS中），但是VM发生切换的时候，还是要更新VM对应的系统寄存器的。

<img src="图片\系统虚拟化09.jpg" style="zoom:80%;" />

* VM Entry

  使用ERET指令从VMM进入VM。

  在进入VM之前，VMM需要**主动加载VM状态**

  * VM内状态：通用寄存器、系统寄存器

    如果发生VM的切换需要重新加载

  * VM的控制状态：HCR_EL2、VTTBR_EL2等 

    通过主动设置寄存器相关位，可以达到控制下陷行为的效果。

* VM Exit

  虚拟机执行敏感指令或收到中断等，以IRQ、FIQ、Exception的形式回到VMM。

  下陷第一步：VMM主动保存所有VM的状态（也只是在寄存器中保留，没有VMCS的内存拷贝）

虽然通过硬件虚拟化，我们似乎可以完全解决CPU虚拟化的问题了，但是还有一个小问题：由于EL2只能运行VMM，不能运行一般的OS内核，所以在这种架构下，**没有办法进行Type-2虚拟化**。除非我们对宿主OS进行大量的修改。

ARMv8.0采用的解决方案是让宿主OS与强烈依赖于其功能的VMM部分运行在EL1，对宿主OS依赖较小的部分可以解耦到EL2。

<img src="图片\系统虚拟化10.jpg" style="zoom:80%;" />

ARMv8.1更进一步，推出VHE，使EL2中可直接运行未修改的宿主OS内核。

<img src="图片\系统虚拟化11.jpg" style="zoom:80%;" />

## 3.内存虚拟化

### （1）概述

为什么需要内存也需要虚拟化呢？

* VM要求自己分配到的“物理内存”是连续增长的
* 隔离性需求，需要一个小管家管理物理内存，防止各个虚拟机发生越界访问。

<img src="图片\系统虚拟化12.jpg" style="zoom:80%;" />

所以内存虚拟化的关键点就是**实现主机物理地址到客户物理地址的翻译机制**。

内存虚拟化的优点：

* 为每个虚拟机提供透明化的从零地址开始连续增长的地址空间，并且地址空间的总和可能大于真正的物理内存空间。
* 小管家的分配机制可以提高内存地址的使用率，减少内存碎片。
* 提高各虚拟机内存之间的安全隔离性。

### （2）翻译机制——硬件虚拟化

<img src="图片\系统虚拟化13.jpg" style="zoom:80%;" />

Intel VT-x和ARM的硬件虚拟化中都扩展了对应的内存虚拟化机制：

* Intel Extended Page Table (EPT)
* ARM Stage-2 Page Table (第二阶段页表) 

在ARM架构中，地址的翻译被分为两个阶段：客户虚拟机内部的客户虚拟地址到客户物理地址的映射，以及客户物理地址到主机物理地址的映射。

<img src="图片\系统虚拟化14.jpg" style="zoom:80%;" />

关于第二阶段页表及其翻译机制，和第一阶段页表的翻译过程十分类似。都是一个多级翻译的过程，每一个有效的页表项都存储了下一级页表页或者内存页的主机物理地址，以及相应的权限。

负责第二阶段翻译的第二阶段页表基地址寄存器由VMM在虚拟机运行前将其对应的第二阶段页表页基地址（主机物理地址）写入VTTBR_EL2寄存器中，并将HCR_EL2系统寄存器的第0位设置为1，这表示打开虚拟机中的第二阶段页表翻译机制。具体的翻译过程无须VMM的介入。

![](图片\系统虚拟化15.jpg)

**具体的翻译过程：**

首先你拿到客户虚拟地址之后第一件事肯定就是去找第一阶段页表基地址寄存器，找到这个寄存器之后，VM会本能的想要直接使用其中的地址，但是这有一个问题，就是这个寄存器实际上是虚拟的，故而其存储的内容本质上是客户物理地址。有小机灵鬼就会问了，客户物理地址怎么了，不可以照样使用吗？在不考虑系统虚拟化的情况下，客户物理地址直接就对应物理内存上的地址，我们借助它作为基地址，就可以顺利找到L0级页表项，之后将其作为L2级页表的基地址，顺藤摸瓜即可。但是在考虑系统虚拟化的情况下，初始的第二阶段页表基地址寄存器以及之后的所谓Lx级页表的页表项，所存储的都是客户物理地址，其只是告诉我们下一级虚拟的页表页的基地址应当去哪里找，比如我们通过L0级页表项拿到了下一级页表页的基地址（客户物理地址），理所当然我们想要去找这个页表页，这就必然会涉及一层客户物理地址向主机物理地址的转化，因为真实内容是存储在主机物理地址上的，故而在每一次第一阶段映射下，都要附加一次第二阶段映射。

> 由于我们发现，这个翻译过程是十分繁琐的，故而ARM架构进行了相关的优化，使得TLB可以直接缓存客户虚拟地址到主机物理地址的映射。

**如何处理缺页异常？**

两阶段翻译的缺页异常分开处理。

* 第一阶段缺页异常
  * 直接调用VM的Page fault handler
  * 修改第一阶段页表不会引起任何虚拟机下陷

* 第二阶段缺页异常
  * 虚拟机下陷，直接调用VMM的Page fault handler
  * VMM此时会根据中断的具体原因（未添加映射还是权限不够）来进行相应的处理，处理完毕之后再恢复虚拟机的正常运行。

## 4.I/O虚拟化

### （1）概述

OS最重要的职责之一就是**管理设备**，基于底层物理设备的基础为上层提供功能性接口。但是如果过VM直接使用物理设备，会带来许多**正确性以及安全性问题**，故而我们需要实现I/O虚拟化。

I/O虚拟化主要有三个功能：

* 为每个VM提供虚拟的外部设备接口。
* 限制VM对真实设备的直接访问，实现I/O数据流和控制流的隔离。
* 提高物理设备的资源利用率。

### （2）软件模拟方法

其本质上是一种**全虚拟化方法**，VM直接使用原生驱动，不要求对驱动程序进行任何修改，I/O虚拟化作用于机器指令层。VMM监视VM的行为，借助**模拟**或**硬件虚拟化**的方法捕捉原生驱动发出的硬件指令，在此基础上为VM提供I/O虚拟化操作。

#### ①相关流程

<img src="图片\系统虚拟化16.jpg" style="zoom:80%;" />

* VM首先基于网络协议栈生成网络包，然后调用网卡驱动。

  接下来就是将网络包中的内容从客户虚拟内存上转移至网卡外设上。

* 网卡驱动以为自己就是最底层的大管家，故而直接通过MMIO操作尝试写入关键信息至内存，然后调用DMA操作移动至网卡外设中。

  但是由于MMIO针对的客户物理地址在第二阶段页表中被映射成了缺页，所以这个操作会触发缺页异常，在硬件虚拟化的支持下直接下陷到KVM中。

* KVM读取下陷的客户物理地址，发现其正在执行DMA写操作，然后直接将相关信息写入与QEMU进程共享的内存区域，将控制流转移至用户态的QEMU。

* QEMU通过KVM交给它的引发MMIO的客户物理地址，发现访问MMIO地址的为某种虚拟网卡。因为QEMU与VM共享内存，故而其就可以直接读取MMIO对应的内存中存储的DMA指令与地址。然后QEMU模拟DMA取出VM内存中的网络包数据，并发起系统调用。

* 宿主机内核将数据发送至物理网卡的驱动程序，并借助物理网卡发送出去，并把控制流返回给用户态的QEMU。

* QEMU发现数据已经发送出去之后，直接向虚拟机中有模有样的插入虚拟中断，以通知其DMA操作已经完成。这个尝试会下陷到KVM，由其具体的执行插入中断的操作，恢复虚拟机的运行。

#### ②设备模拟的优缺点

* 优点
  * QEMU可以模拟任意设备
  * 允许在中间拦截（Interposition）：例如在QEMU层面检查网络内容
  * 不需要硬件修改

* 缺点

  来回下陷，可谓是七进七出，大大影响了性能。

### （3）半虚拟化方式

软件模拟的方法之所以性能交较差，关键点在于它太自欺欺人了。每次DMA都要由硬件虚拟化捕捉，先下陷到KVM，然后KVM在转到QEMU，QEMU再读取数据转移到内核网卡驱动...半虚拟化则直接打开天窗说亮话，VM“知道”自己运行在虚拟化环境，VM内运行**前端(front-end)驱动**，VMM内运行**后端(back-end)驱动**。VM可以直接将设备调用的信息写入到某共享内存中，然后使用VMM提供的超级调用，进行处理即可。

<img src="图片\系统虚拟化17.jpg" style="zoom:80%;" />

流程与上大致类似，其性能提升主要源自于两个方面：

* 全虚拟化需要使用MMIO接口，多次MMIO访问只能传输少量数据。半虚拟化中通过前后端驱动共同维护的IO队列，可以实现**批处理**，这些数据的传输只需一次超级调用。

* 半虚拟化方法中同一类设备通常共享一组前后端驱动，不再需要为具体的设备维护对应的驱动，因而大大减少了驱动的数目。

  > 因为半虚拟化驱动数目较少且简单，故而另一种半虚拟化I/O的实现方法就是直接将后端驱动直接运行在EL2中，这种架构减少了KVM和QEMU之间的特权级切换次数，能够进一步提升I/O虚拟化的性能。

除此之外，其还具有VMM实现简单的优点，因为VMM不再需要理解物理设备接口。但是缺点就是需要修改虚拟机操作系统的内核代码。

## 5.案例：KVM/QEMU

* QEMU运行在用户态，负责实现策略。

  也提供虚拟设备的支持

* KVM以Linux内核模块运行，负责实现机制。

  可以直接使用如内存管理、进程调度的Linux内核功能

  使用硬件虚拟化功能

* 两部分合作
  * **KVM捕捉所有敏感指令和事件**，传递给QEMU
  * KVM不提供设备的虚拟化，需要**使用QEMU的虚拟设备**。

<img src="图片\系统虚拟化18.jpg" style="zoom:80%;" />

<img src="图片\系统虚拟化19.jpg" style="zoom:80%;" />



















