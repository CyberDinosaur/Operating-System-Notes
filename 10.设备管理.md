# 一.概述

“计算机”顾名思义...显然不是仅仅用来算数的。根据不同的需求和场景，人们发明了大量的专用设备：通信、存储、智能计算、安全协处理器等。由于厂商不同，各种设备具有不同的协议、规范、接口，我们直接进行使用可能会十分的繁琐。**OS如何对千变万化的外设进行统一管理，并向上提供功能接口？**

# 二.设备类型抽象

计算机上的软件不能与外设直接进行通信，因为外设的崩溃可能会导致整个系统的崩溃，软件的恶意操作也有可能会对外设造成损害，加之外设进行更新可能会导致软件对其利用过程变得十分复杂。

因此OS需要对设备进行**抽象**，将不同外设细节和协议封装在统一接口的内部，使得外设的抽象实例可以通过接口对物理设备进行操作，而软件可以借助OS调用外设的抽象。

<img src="图片\设备管理01.jpg" style="zoom:80%;" />

在Linux操作系统中有3种常见的设备抽象。

## 1.字符设备（char）

* **例子**

  LED、键盘、串口、打印机、USB设备、显存、命令行终端等

* **访问模式**

  将设备上的信息抽象为连续的字节流，应用程序通常以**顺序访问方式**对字符设备进行**字节粒度**的读写。

  应用程序对字符设备的读写会直接触发驱动对设备的I/O操作。

* **通常使用文件抽象：**

  * open()
  * read()
  * write()
  * close()

## 2.块设备（block）

* **例子**

  磁盘、U盘、闪存等（以存储设备为主）

* **访问模式**

  以**随机访问**的方式以**块粒度**进行读写

  应用程序对块设备的读写不会直接触发驱动对设备的I/O操作，而是在驱动程序之上增加一层缓冲（页缓存），避免和慢设备频繁交互。

## 3.网络设备（network）

* **例子**

   以太网、WiFi、蓝牙等（以通信设备为主）

* **访问模式**

  处理的数据单位是**网络包**，面向**格式化报文**的收发。

  OS在内核维护着复杂的协议栈，负责对网络包进行封装、解析、寻址等处理，以支持不同功能的通信。

  应用程序对字符设备的读写会直接触发驱动对设备的I/O操作。

* **通常使用文件抽象：**

  **设备的接口抽象——套接字**

  * socket()
  * send()
  * recv()
  * close()

# 三.设备与OS的交互

我们对设备进行抽象的不是for fun的，我们最终的目标是让计算机系统中的软件能够根据自身需求调用各种外设，而OS就是起到向下统一管理所有设备，向上给用户提供统一的接口。

## 1.设备连接

设备通过**总线**与CPU相连，而且设备和总线的工作频率一般低于CPU。

###（1）AMBA总线

ARM架构下的片上总线规范称为高级微控制器总线结构（Advanced Micro-controller Bus Architecture, AMBA），其定义乐ARM架构片上系统Soc的通信标准。

* 高级高性能总线AHB：用于连接其它高性能IP核、片上和片外内存以及中断控制器等高性能模块。
* 高级系统总线ASB：用于某些不必要使用AHB但同时又需要高性能特性的芯片中，能起到一部分降低功耗的作用。
* 高级设备总线APB：用于连接低速的设备，作为低功耗的精简接口总线。

### （2）PCI总线

设备组件互联标准PCI（Peripheral Component Interconnect）使得任何厂商生产的设备只要符合对应规范，都可以通过PCI插槽与CPU进行通信。

当数据传输速率过高时，PCI所采用的并行线路间会相互干扰，PCIe使用了基于数据包的串行连接协议，相比PCI带宽更高，同时能够前向兼容PCI设备驱动

## 2.设备与OS通信

### （1）可编程I/O

CPU以**读写设备寄存器**的方式与其进行通信。

* 控制寄存器：用来接收来自驱动程序的命令
* 状态寄存器：用来反馈当前设备的工作状态
* 输入/输出寄存器：用于驱动和设备之间的数据交互

CPU对设备寄存器的访问一般有两种方式：

* **内存映射I/O（Memory-Mapped I/O，MMIO）**

  将设备的寄存器直接映射到内存空间上，并拥有独立的地址。CPU可以通过内存读写指令直接与设备进行交互。

  > 行为与内存不完全一样，读写会有副作用

* **端口映射I/O（Port-Mapped I/O，PMIO）**

  CPU通过专门的端口操作指令与设备进行交互。

以上两种交互方式都称为可编程I/O（Programmed I/O, PIO）

###（2）直接内存访问DMA

PIO的逻辑十分简单，就是CPU以不同的形式对设备寄存器进行操作，其需要CPU主动承担数据搬运的工作。那么有没有什么方式能够解放CPU，尽量减少I/O对CPU时间片的占用呢？

**直接内存访问机制**（Direct Memory Access, DMA）允许设备绕过处理器直接读写系统内存，从而解放CPU去做更加有意义的事情。DMA可以由CPU发起，也可以由外设发起。

####①通信机制

<img src="图片\设备管理02.jpg" style="zoom:80%;" />

1. **CPU向DMA控制器发送DMA请求。**

   包含**方向**（数据是从内存流向外设，还是由外设流向内存）、**DMA缓冲区的位置和长度**（也就是读写的物理内存范围）。之后CPU放弃对总线的控制权。

2. **DMA控制器获得总线控制权，之后根据从处理器获得的指令，进行内存与外设之间的数据传输，在这期间处理器可以执行其它任务。**

3. **DMA控制器完成DMA后向CPU发出中断信号，通知其DMA已经完成，CPU会重新获得总线的控制权。**

####②DMA类型

如果DMA控制器是与CPU和物理内存一同连接到系统总线上的模块，称其为第三方DMA，也即标准DMA。

如果DMA控制器由设备自身来担任，相当于外设可以直接获得总线的控制权进行DMA操作。称为第一方DMA，也称为总线控制。

需要注意的是，第一方DMA的情况下，如果同时有多个设备进行DMA，**总线控制器**需要进行仲裁，决定优先次序，同一时间只能由一个设备进行DMA。而在第三方DMA的情况下，只要是由设备发出的DMA请求都由DMA控制器进行管理，其同时掌握总线控制权，**DMA控制器**就可以按照一定的策略对所有的设备DMA请求进行调度。

#### ③设备地址翻译IOMMU

从上面我们能看到朴素的DMA是十分简单的，不过就是转让总线控制权给DMA控制器或者设备自身（第三方DMA多了一层抽象），让其自己进行数据的传输，传输完毕后借助中断通知CPU一声即可。

但是由于总线或一些设备自身的限制，导致设备能够表征的地址空间大小要远远小于内存空间大小（甚至比DMA缓冲区的大小还小）。比如某设备地址编码长度为24位，其能表征的范围为16MB。这就会造成外设无法表示出对应的DMA缓冲区范围。

<img src="图片\设备管理03.jpg" style="zoom:80%;" />

一种解决方案就是**回弹缓冲区**，实际上就是不断进行缓冲区的拷贝、平移。

另一种解决方案就是在外设地址空间与内存地址空间增加一层映射——IOMMU，其负责**将设备的总线地址翻译成对应的内存地址**。类似于MMU的作用。

* 解决外设的总线地址空间与DMA缓冲区大小、位置不匹配的问题。
* 免除了DMA缓冲区必须连续的限制，有了中间层映射器IOMMU，DMA缓冲区可以随意分配。
* 为OS提供内存访问保护机制，可以阻挡恶意设备以及驱动对内存的非法访问。

#四. OS如何响应设备：中断

<img src="图片\设备管理04.jpg" style="zoom:80%;" />

## 1.为什么需要中断控制器？

尽管CPU可以通过PIO或者DMA方式完成内存与外设的数据传输（区别无非就是是否为CPU自身操作），但是由于设备的状态对于CPU来说是不可见的，所以CPU很难知晓何时进行下一步。

一种解决方法就是CPU不断**轮询**设备的状态，但是这样会导致大量计算资源的浪费与性能的损耗。另一种方法就是引入**中断(interrupt)**，让设备主动告知CPU一个外部事件的发生。我们本节的重点就是中断机制的实现。

中断机制的一个关键点就是CPU控制流的切换，也即中断发生之后，**CPU需要以某种途径接收中断信号、处理中断信号**。

中断信号的接收在之前是利用中断引脚来进行的，每个设备对应一个中断引脚对应一个物理地址。这种设计随着外设数量越来越多逐渐变得不可行，而且引脚并没有指定中断的优先级，难以对各种中断进行管理，所以**中断控制器(interrupt controller)**就出现了，其相当于**专门管理设备中断的中间层**，负责将不同的中断信号交由对应的**中断处理函数(IRQ handler)**进行处理。

## 2.中断的分类

AArch64的中断可以分为三类：

* IRQ（Interrupt Request）：**普通中断**，优先级低，处理慢

* FIQ（Fast Interrupt Request）：**快速中断**，优先级高，处理快。而且一次只能有一个FIQ，常为可信任的中断源预留。

  > IRQ与FIQ的优先级区别通过连接不同的CPU引脚来实现。

* SError（System Error）：**原因难以定位、较难处理的异常**，多由异步中止（Abort）导致，如从缓存行（Cacheline）写回至内存时发生的异常。

##3.中断控制器的机制

ARM架构下的中断控制器一般有两种：ICOLL与GIC。

ICOLL主要用于早期的ARM芯片设计上，其支持128个中断号，根据中断号对应的中断信息可以得到中断的处理优先级。其具体的处理是借助**中断向量表**来实现的。

GIC称为**通用中断控制器**（Generic Interrupt Controller, GIC），用于ARMv8。

### （1）GIC的总体结构

<img src="图片\设备管理05.jpg" style="zoom:80%;" />

GIC的关键功能部分实际上只有两个：**分发器接口**(distributor interface)和**CPU接口**(CPU interface)。

* 分发器接口负责管理所有的中断：收集所有的中断请求，并根据中断的优先级按照一定的策略分发给CPU核（多核路由）。
* CPU接口直接与CPU核相连，其将分发器给予的中断进行相关信息的配置（是否启用中断、配置处理优先级、是否确认中断请求、是否通知GIC完成中断处理），然后交给CPU内核并请求相关的处理程序。

###（2）GIC处理中断的总体流程

GIC会对中断类型进行分类，同时给各个类型分配一些中断号区间以方便管理，每个中断号都对应**一个中断**。GIC对每个中断号都维护了四种状态：

* Inactive：此中断处于无效状态，没有中断到来。
* Pending：此时中断已经发生，但是CPU还没有进行响应（可能是正在处理同样的中断请求）。
* Active：中断已经被响应且正在被处理。
* Active&Pending：有相同中断号的请求发生。

某个中断源产生中断，**传递给GIC**，此时对应中断号的状态由Inactive变为Pending；之后GIC根据其优先级将其分发到对应的CPU接口进行信息配置，然后被传递给CPU核，**在CPU调用中断处理函数响应并处理该中断时**，中断才会由Pending转化为Active；最终CPU处理完毕，**向GIC发送EOI信号(End of Interrupt)通知GIC中断处理完毕**，GIC才会将对应中断的状态调整为Inactive。

### （3）中断优先级

对于中断，我们的目的肯定是响应得越快越好，设备恨不得在发出中断信号的瞬间，GIC就收到CPU回传的EOI信号。但是对于这样一个**有限资源、请求无限**的模型，这显然是不可能实现的，而且其必定要涉及到中断信号的调度策略，也即GIC的分发器如何选择下一个应当分发的中断信号？

如果一个中断号同时发生多个中断，GIC会首先分发**高优先级**的中断给CPU接口。

<img src="图片\设备管理06.jpg" style="zoom:80%;" />

此时就会面临一个问题，如果CPU在处理当前中断的时候，有**更高优先级的中断产生**或者有**相同优先级的中断产生**，GIC应当如何处理？

* **屏蔽全局中断：不再响应任何外设请求。**

  更倾向于中断处理的**原子性**，但是如果中断处理时间较长，可能会导致其它中断长时间无法响应，特别是CPU实时任务较多，导致**响应时间过长**。

* **屏蔽对应中断：只对相同优先级的中断停止响应。**

  通常情况下的处理方式，**允许较高优先级的中断进行抢占，相同优先级则无法进行抢占**，对系统的整体影响最小。

  > 以ARM架构为例，IRQ在执行的时候可以被FIQ抢占，FIQ被处理的时候无法被抢占。

> 思考：在两台主机的通信场景中，如果发送端拼命发送数据，接收端网卡的数据应当被如何传输至OS内？
>
> 如果是轮询，很有可能导致接收端还没有处理，之前的数据就被覆盖了；如果采用中断，数据到来时网卡会向GIC传输中断信号，在被响应之前，网卡无法接收新来的数据==？==
>
> 对策：不让发送端传输的太快，从而避免丢包。
>
> * 静态配置法：UART驱动需要在初始化时指定“波特率”
> * 动态协商法：TCP设计了流量控制机制，发端逐步试探出收端收包能力的上限
>
> 在UDP之下采用中断处理，用户会收到什么数据包（last or first?）——==first==

###（4）中断处理过程

这一部分实际上就是探究中断信息交给CPU核之后，其应当如何处理。

#### ①大致流程

<img src="图片\设备管理07.jpg" style="zoom:80%;" />

中断在CPU内的总体处理流程是很固定的：抢占CPU当前正在执行的任务（因为中断能通过CPU接口进入内核就代表CPU此时可以处理中断，故而一定会抢占），CPU进入内核态，查询异常向量表（Exception Vector Table）找到对应的**中断处理函数**（Interrupt Service Routine, ISR）进行中断处理。

问题的关键在于如何设计中断处理函数？

#### ②中断处理函数的设计

由于ISR进行的时候，其它用户任务没有办法进行，而且由于同优先级的中断会被屏蔽，也导致外面的GIC无法分发中断，如果设备缓冲区不足，会导致中断丢失。故而**ISR这部分应当越快越好（也即响应时间越短越好）**。

为了达成这个目的，我们可以：提高处理速度or减少处理的任务量：

* ISR处理速度主要由硬件基础以及其它操作系统结构所决定，难以通过ISR的设计优化来改变。
* ISR处理的任务量可以减小，使其只处理必要的部分（上半部），其它的非关键操作可以推迟到后期完成（下半部）。

#### ③中断处理函数的机制

对中断要处理的任务进行轻量化处理，分为**上半部**和**下半部**：

* 上半部（Top Half）：必要或时延要求高的工作

  上半部也就是**中断处理函数ISR**需要做的操作，也就是所谓的**硬中断**。

  其通过**异常向量表**调用合适的由硬件驱动提供的中断处理handler (处理硬中断），并且向系统注册新的处理任务，将其作为下半部的执行函数。

  上半部应当尽量轻量化处理。

* 下半部（Bottom Half）：剩余非关键、复杂且时延要求较低的工作。

  下半部是由硬中断注册的任务，相对较复杂且时延要求较低，相当于**具有较高优先级的内核任务**，由OS调度器管理。

当CPU接口将中断交给CPU核时，原有的任务被打断，CPU会查询异常向量表找到对应中断的处理函数ISR（也即硬中断处理函数），ISR处理关键且时延要求较高的任务并完成对下半部任务的注册，ISR处理完毕之后CPU响应GIC表明该中断事件已经处理完成并关闭中断屏蔽，从而允许CPU继续对到来的中断进行响应。下半部任务将借助某种机制延迟处理，由OS调度器进行调度。

现在我们只需要搞清楚下半部延迟处理究竟是如何做到的即可！

#### ④下半部处理机制

##### A.软中断（Softirqs）

就是把下半部看做普通的内核任务，一般在上半部之后进行。

* 不会中断当前的CPU任务，等待调度器的**主动调度**。

  * 一般硬中断处理完成后，内核就会检查当前CPU是否有待处理的软中断，有就立即执行。
  * 为了避免用户态任务饥饿，Linux会限制软中断连续处理的最长时间和最大次数，一旦超过这个限制就会将软中断追加到各个CPU特有的ksoftirqd内核线程中，ksoftirq的优先级被设为最低，故而不会影响其它任务执行。

* 和硬中断一样也有优先级之分。

* Softirq运行时可以被中断抢占。

* 可以在多核上**并发执行**

  故而要求软中断的处理函数必须是**可重入**的或者根据需要加锁。

  > 所谓的可重入就是指一个函数允许在它被调用结束之前再次被调用而不会出错（也即可以安全地并发执行，无需设置临界区）。一般可重入函数不使用静态数据或者全局非常量数据。
  >
  > 硬中断的处理函数之所以不要求可重入是因为其在响应的过程中，屏蔽所有到来的硬中断（不考虑中断嵌套），因此只会串行执行。

##### B.tasklet机制

软中断的问题是其只能在代码编译时**静态分配**==？==，而无法根据需要在运行时进行动态创建。为了解决这个问题，Linux基于软中断实现了tasklet机制，其也**运行在软中断上下文中**。

<img src="图片\设备管理08.jpg" style="zoom:80%;" />

* 基于Softirqs，但**可以被动态创建和销毁**

* 同一时期，同种类型的Tasklet实例一次只能运行一个，也即**同种类型的Tasklet无法并发执行**；不同类型的Tasklet可以同时运行在不同CPU上。

  由于不支持同类型的tasklet实例并发执行，本身就具有**原子性**，故而就没有同步的必要，对应的处理函数自然也没必要规定为可重入的。

软中断侧重于中断的处理效率（故而支持并发），tasklet侧重编程开发的友好性，其使用者无需考虑同步加锁和代码可重入问题。但是因为tasklet依托于中断上下文，执行期间不能睡眠，外加设计上的不可抢占性，导致其可能引起难以预测的系统延迟，严重的话甚至会影响系统的整体实时性。

##### C.工作队列（Work Queues） ==待补充==

Softirq和Tasklet使用中断上下文，而工作队列**使用进程上下文**(单独创建一个进程负责处理中断函数) ，故而其可以睡眠。

方式：

* 在内核空间维护一个FIFO队列，**workqueue内核进程**不断轮询队列。
* 中断负责责enqueue(fn, args)， workqueue负责dequeue并执行fn(args) 

特点：

* 只在内核空间，不和任何用户进程关联，没有跨模式切换和数据拷贝

# 五.OS如何管理设备

## 1.驱动程序概述

OS中负责控制设备的是较为底层的定制化程序——驱动。

<img src="图片\设备管理09.jpg" style="zoom:80%;" />

驱动向上与OS内核其余部分交互，处理相应的指令，系统接口(ioctl)就是让用户空间的应用通过驱动间接和设备进行交互的接口，负责接收相关指令；根据用户的指令，驱动向下控制设备，其通信方式就是我们前面提过的设备与OS通信的PIO或DMA。

> 某种程度上，OS就是CPU的“驱动”（最大的管家）

##2.宏内核vs微内核的驱动

<img src="图片\设备管理10.jpg" style="zoom:80%;" />

## 3.设备驱动模型

 设备的整体趋势：

* 数量、规模和种类越来越多
* 版本更新速度越来越快

这就导致驱动代码量在快速增长，而驱动开发程序员的头发却迅速减少，就是为了解决这个问题而出现的。其表征OS对驱动的统一抽象，OS事先规定好一组数据结构和应该实现的接口，从而驱动开发就变成了**对数据结构的填充以及接口函数的挂载**。

**驱动模型Linux Device Driver Model (LDDM)：**

设备驱动模型的数据结构对象对应于系统拓扑结构的各个部分，各对象的关系也体现了不同设备之间的依赖关系，是体系结构整体的框架。

<img src="图片\设备管理11.jpg" style="zoom:80%;" />

Linux设备驱动模型定义了四种基本的数据结构：

* **设备（device）**：用于抽象系统中所有的硬件，包括CPU和内存——表示有什么
* **总线（bus）**：用于抽象I/O设备和CPU之间的通信——表示怎么连
* **类（class）**：具有相似功能或属性的设备集合，旨在抽象出一套可以在多个设备间共享的数据结构和接口。
* **驱动（driver）**：用于抽象控制设备的驱动程序。结构体中包含多个函数指针，也即开发人员需要关注的接口——表示怎么用

> 总线和类可以看成在两个不同层面上对设备的组织和管理：总线是在拓扑结构上对设备进行组织，而类则是在逻辑结构上对设备进行组织。

Linux还提供了基于内存文件系统的sysfs虚拟文件系统，用于将内核中的设备信息和驱动信息以文件的形式提供给用户程序使用。

LDDM特点：

* device为bus_type 、device_driver父类的多继承，所以要实例化一个device，先实例化父类driver和bus。（非常符合直觉）
* device_driver和device都注册到bus上。bus_type上的**match()**方法可以匹配设备与驱动，如果匹配成功就调用device_driver的**probe()**对驱动进行初始化：probe成功后内核生成设备实例，驱动注册的file_operations可以被应用程序所访问。
* 提供**设备资源管理框架**，驱动开发者只需要申请资源，资源的回收和释放则交给设备资源管理框架自动完成。
* 支持电源管理与设备的热拔插。

## 4.设备树

由于ARM架构外设的种类数不胜数，配置信息各不相同，所以把硬件配置**硬编码**到系统代码中会极大增加OS的维护成本。

设备树Device Tree：描述硬件信息的数据结构，只有不能动态探测的设备才会出现在设备树中。

**设备树源码(Device Tree Source ，DTS)以文本的形式将计算机系统设备描述为树型结构，每种设备都存在一个节点描述并且挂载在根节点之下。**

<img src="图片\设备管理12.jpg" style="zoom:80%;" />

Linux在启动阶段由**引导程序**加载并启动内核，同时把存放**设备树二进制文件**的地址传递给内核。Linux内核根据设备树**注册各节点所代表的设备**，根据compatible属性后接的字符串内容**识别设备并匹配对应的驱动代码**，进而实现对设备的管理。

> x86一般使用高级配置与电源接口（Advanced Configuration and Power Interface，ACPI）来识别设备。其是设备和OS之间的一层抽象，统一地向OS汇报硬件设备的情况，同时提供管理设备的接口和方法。









